---
title: "PCA for MPA Attribute Data"
author: "Cori Lopazanski"
date: '2022-06-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

*Goal*: Explore whether key characteristics describe variation among MPAs. Can
we reduce the attribute data to fewer variables while still capturing the
key information?

*Steps*
1. Log-transform the continuous data: standardize across variables with different scales
2. Examine pairwise correlations across variables
3. PCA with all continuous variables


# Setup

```{r}
# Packages
library(tidyverse)
library(here)
library(ggbiplot)
library(GGally)
library(vegan)

# Directories
base.dir <- "/Volumes/GoogleDrive-105151121202188525604/Shared drives/NCEAS MPA network assessment/MPA Network Assessment: Working Group Shared Folder/data/sync-data" # Cori Local
#base.dir <- "/home/shares/ca-mpa/data/sync-data/" #Aurora Base
repo.dir <- here::here("analyses", "7habitat")
in.dir <- here::here(base.dir, "mpa_traits", "processed")

# Read Cleaned Data
attributes <- read_csv(file.path(in.dir, "mpa_attributes_clean.csv"))
```

# Processing

## Minor Corrections 

```{r}
data <- attributes %>% 
  mutate(four_region_north_ci = as.factor(four_region_north_ci)) %>% 
  mutate(prop_hard = total_hard_substrate/size_km2) %>% 
  mutate(prop_soft = total_soft_substrate/size_km2)
```


## Log-Transform Continuous Data

Otherwise, variables with high variances (e.g. depth range) will have higher loadings in the principal components.

```{r}
log_data <- data %>% 
  select(name, mpa_class, four_region_north_ci,
         age_yr,
         distance_to_port:max_depth_m, 
         asbs_overlap:hardened_armored_shore_km, 
         citizensci_inaturalist_obs, 
         total_hard_substrate:prop_soft) %>% 
  select(!max_kelp_canopy_landsat_km2) %>% # Drop because too many NAs 
  select(!larval_connectivity) %>%  # Drop because too many NAs
  drop_na() %>% 
  mutate_if(., is.numeric, log1p)
```

## Consider: Converting Factors to Dummy Variables

The four regions could be considered characteristics we want, so would it be useful to convert them to dummies?

# Pairwise Correlations

```{r}
# Warning: this is hectic and takes a minute
#ggpairs(log_data[4:39], aes(col = log_data$four_region_north_ci, alpha = 0.8))

# Some of the data is still very skewed due to high number of zeroes
# Some also has pretty high correlation - e.g. depth
```

# PCA: All Continuous Variables

```{r}
pca <- prcomp(log_data[4:42],
              center = TRUE,
              scale = TRUE) # normalize with standard deviation 1

print(pca)
summary(pca)

plot(pca, type = "lines")

# Plot
ggbiplot(pca, group = log_data$four_region_north_ci, ellipse = T) +
  theme_minimal() +
  labs(title = "All Trait Variables")

# Principal component loadings
loadings <- as.data.frame(pca$rotation)

ggbiplot(pca, group = log_data$mpa_class, ellipse = T) +
  theme_minimal() +
  labs(title = "All Trait Variables")
```

## Explore Explained Variance
```{r}
# Retrieve the standard deviation of each PC
stdev <- pca$sdev

# Calculate variance of each PC
var <- stdev^2

# Calculate proportion of variance explained by each PC
prop_var_ex <- var/sum(var) 
prop_var_ex[1:20]

```
```{r}
# Scree plot shows we get past 98% var explained ~ PC 25
plot(cumsum(prop_var_ex), 
     xlab = "PC", 
     ylab = "Cumulative proportion of variance explained", 
     type = "b")
abline(h = 0.98)

# Can also get this through summary
summary(pca)
```

## Reducing Variables

PCA aims to estimate the contribution of different variables to the variation among the points - if there is high correlation between some of the variables, that can result in a higher weight of those variables in the estimation of the components.

Based on my understanding, this is something that you wouldn't want to manipulate if these were variables signifying different things about the data, however if you have multiple variables that are measuring the same "underlying" aspect of the data... e.g. multiple variables estimating hard substrate (maybe?) that it could be worthwhile to discard them.

### Revisit Correlations

```{r}
# Reminder that this takes a minute or two
ggpairs(log_data[4:10], aes(col = log_data$four_region_north_ci, alpha = 0.8))
```

### Decisions

- Remove minimum and maximum depth, and just keep depth_range
- Remove all the different hard/soft calculations and just keep total hard/soft
- Reduce duplicates among habitat types
- Remove random non-habitat characteristics (asbs overlap, inat obs)

```{r}
log_data_reduced <- log_data %>% 
  select(name, 
         mpa_class, 
         four_region_north_ci, 
         distance_to_port, 
         size_km2, 
         shore_span_km, 
         historical_protection_overlap,
         sandy_beach_km, 
         rocky_inter_km, 
         offshore_rock_km, 
         max_kelp_canopy_cdfw_km2,
         coastal_marsh_km,
         tidal_flats_km,
         total_hard_substrate,
         total_soft_substrate,
         depth_range)
```

From 40 variables to 18 variables


## Reduced PCA

```{r}
pca_reduced <- prcomp(log_data_reduced[4:16],
                      center = TRUE,
                      scale = TRUE) 

summary(pca_reduced)

plot(pca_reduced, type = "lines")

ggbiplot(pca_reduced, group = log_data_reduced$four_region_north_ci, 
         ellipse = T) +
  theme_minimal() +
  labs(title = "Reduced Trait Variables")

# Principal component loadings
loadings_reduced <- as.data.frame(pca_reduced$rotation)

# Re-graph with groupings as MPA type instead?
ggbiplot(pca_reduced, group = log_data_reduced$mpa_class, 
         ellipse = T) +
  theme_minimal() +
  labs(title = "Reduced Trait Variables")
```


```{r}
# This is hacky and I'm not sure if it's recommended
# See which variables contribute the most...
loadings_reduced$total <- rowSums(abs(loadings_reduced[2:13]))
loadings_reduced$total_first10 <- rowSums(abs(loadings_reduced[2:11]))

loadings$total <- rowSums(abs(loadings[2:39]))
loadings$total_first10 <- rowSums(abs(loadings[2:11]))
```


# NMDS ?

NMDS can have some value over PCA because it uses rank orders rather than absolute distance, so can also use different types of data.

```{r}
nmds_data <-  data %>% 
    select(name, mpa_class, four_region_north_ci,
         age_yr,
         distance_to_port:max_depth_m, 
         asbs_overlap:hardened_armored_shore_km, 
         citizensci_inaturalist_obs, 
         total_hard_substrate:prop_soft) %>% 
  select(!max_kelp_canopy_landsat_km2) %>% # Drop because too many NAs 
  select(!larval_connectivity)  # Drop because too many NAs
  
nmds <- metaMDS(nmds_data[4:42], k = 2)
```


# References

Useful guide to PCA analyses: https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/
https://www.datacamp.com/tutorial/pca-analysis-r

Explanation for whether to reduce variables before doing PCA:
https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca


